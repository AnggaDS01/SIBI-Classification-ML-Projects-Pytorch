{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import collections\n",
    "\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from SIBI_classifier.logger.logging import log_manager\n",
    "from SIBI_classifier.utils.main_utils import custom_title_print\n",
    "from SIBI_classifier.configuration.configuration import ConfigurationManager\n",
    "from SIBI_classifier.exception import SIBIClassificationException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SETUP CONSTANTS\n",
    "In this section, we will define some constants that are important for data processing and model training.\n",
    "These constants will help us set the dataset folder path, the extension pattern of the image files to be collected, and which folders to access.\n",
    "Later, these constants will be used in various parts of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[ 2024-11-29 18:22:50 ] create_directories_logger\u001b[0m - \u001b[32mINFO\u001b[0m - created directory at: \u001b[96martifacts\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:50 ] create_directories_logger\u001b[0m - \u001b[32mINFO\u001b[0m - created directory at: \u001b[96martifacts/<model_name>/data_preprocessing/objects\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:50 ] create_directories_logger\u001b[0m - \u001b[32mINFO\u001b[0m - created directory at: \u001b[96martifacts/<model_name>/data_ingestion/SIBI_dataset\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f002bd755b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG = ConfigurationManager()\n",
    "DATA_PREPROCESSING_CONFIG = CONFIG.get_data_preprocessing_config()\n",
    "DATA_INGESTION_CONFIG = CONFIG.get_data_ingestion_config()\n",
    "LOGGER = log_manager.setup_logger(\"DataPreprocessingLogger\")\n",
    "COLLECT_AND_COMBINE_IMAGES_LOGGER = log_manager.setup_logger(\"collect_and_combine_images_logger\")\n",
    "FILE_PATH_INFO_LOGGER = log_manager.setup_logger(\"file_path_info_logger\")\n",
    "SPLIT_TRAIN_VALID_TEST_LOGGER = log_manager.setup_logger(\"split_train_valid_test_logger\")\n",
    "CLASS_DISTRIBUTION_LOGGER = log_manager.setup_logger(\"class_distribution_logger\")\n",
    "COLOR_TEXT = \"yellow\"\n",
    "\n",
    "np.random.seed(DATA_PREPROCESSING_CONFIG.seed)\n",
    "random.seed(DATA_PREPROCESSING_CONFIG.seed)\n",
    "torch.manual_seed(DATA_PREPROCESSING_CONFIG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPARING DATA WORKFLOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data From Folders\n",
    "\n",
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_images_with_regex_and_count(\n",
    "        path, \n",
    "        folder_classes, \n",
    "        extensions_pattern\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Collects images from specified directories that match a given file extension pattern.\n",
    "\n",
    "    Args:\n",
    "        path (str): The root directory path containing the folder classes.\n",
    "        folder_classes (list): List of folder names representing different classes.\n",
    "        extensions_pattern (str): Regex pattern to match file extensions.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are folder classes and values are lists of image paths.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize a dictionary to hold image paths for each class\n",
    "        image_paths = {folder_class: [] for folder_class in folder_classes}\n",
    "        \n",
    "        # Compile the regex pattern for matching file extensions\n",
    "        pattern = re.compile(str(extensions_pattern), re.IGNORECASE)\n",
    "\n",
    "        # Iterate over each class folder\n",
    "        for folder_class in folder_classes:\n",
    "            folder_path = Path(path) / folder_class\n",
    "\n",
    "            # Recursively search for files matching the pattern in each class folder\n",
    "            for file_path in folder_path.rglob(\"*\"):\n",
    "                if pattern.search(file_path.suffix):\n",
    "                    image_paths[folder_class].append(file_path)\n",
    "\n",
    "            COLLECT_AND_COMBINE_IMAGES_LOGGER.info(f\"Collecting from class {log_manager.color_text(folder_class, COLOR_TEXT)}: {log_manager.color_text(len(image_paths[folder_class]), COLOR_TEXT)} paths found.\")\n",
    "\n",
    "        return image_paths\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle exceptions and return an empty dictionary if no classes are retrieved\n",
    "        COLLECT_AND_COMBINE_IMAGES_LOGGER.info(f\"No classes are retrieved from directory.\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_images(\n",
    "        image_paths, \n",
    "        num_samples, \n",
    "        seed=42\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Retrieves a random number of images from the image path list.\n",
    "    Args:\n",
    "        image_paths (list): A list of image paths.\n",
    "        num_samples (int): The number of images to retrieve. If None, all images will be selected.\n",
    "        seed (int): Seed to control the random retrieval results so that the results can be reproduced. Default is 42.\n",
    "    Returns:\n",
    "        list: A list of randomly selected image paths.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        random.seed(seed)\n",
    "        return random.sample(image_paths, min(len(image_paths) if num_samples is None else num_samples, len(image_paths)))\n",
    "\n",
    "    except Exception as e:\n",
    "        raise SIBIClassificationException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_combine_images(\n",
    "        classes, \n",
    "        train_path=None, \n",
    "        valid_path=None,\n",
    "        test_path=None, \n",
    "        pattern_regex=r\"\\.(jpe?g)$\", \n",
    "        num_images_per_class=None, \n",
    "        seed=42\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Collects and merges images from the training and validation folders, and retrieves a random number of images from each class.\n",
    "    Args:\n",
    "        classes (list): List of classes (folder names) to process.\n",
    "        train_path (str): The main path of the training folder that contains image data sub-folders.\n",
    "        valid_path (str): The main path of the validation folder that contains the image data sub-folders.\n",
    "        pattern_regex (str): The regex pattern for matching image file extensions (e.g. r'\\.(jpg|png|jpeg)$').\n",
    "        num_images_per_class (dict): Dictionary containing the number of images to fetch for each class. If None, all images will be retrieved.\n",
    "        seed (int): Seed for random image retrieval. Default is 42.\n",
    "    Returns:\n",
    "        list: A combined list of image paths from the training and validation folders that were randomly picked.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        def process_class(cls):\n",
    "            # Combine images from training and validation for each class\n",
    "            all_train_images = train_images_paths.get(cls, [])\n",
    "            all_valid_images = valid_images_paths.get(cls, [])\n",
    "            all_test_images = test_images_paths.get(cls, [])\n",
    "            all_combined_images = all_train_images + all_valid_images + all_test_images\n",
    "\n",
    "            # Retrieve a random number of images from the combined image\n",
    "            return get_random_images(\n",
    "                image_paths=all_combined_images,\n",
    "                num_samples=None if num_images_per_class is None else num_images_per_class.get(cls, len(all_combined_images)),\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "        custom_title_print(f\"COLLECT {classes} FROM TRAINING DATA\")\n",
    "        train_images_paths = collect_images_with_regex_and_count(train_path, classes, pattern_regex)\n",
    "        custom_title_print(f\"=\")\n",
    "\n",
    "        # Print the title for the image collection process of the validation data\n",
    "        custom_title_print(f\"COLLECT {classes} FROM VALIDATION DATA\")\n",
    "        valid_images_paths = collect_images_with_regex_and_count(valid_path, classes, pattern_regex)\n",
    "        custom_title_print(f\"=\")\n",
    "\n",
    "        # Print the title for the image collection process of the test data\n",
    "        custom_title_print(f\"COLLECT {classes} FROM TEST DATA\")\n",
    "        test_images_paths = collect_images_with_regex_and_count(test_path, classes, pattern_regex)\n",
    "        custom_title_print(f\"=\")\n",
    "\n",
    "        # Print titles for the process of merging images from training and validation\n",
    "        custom_title_print(f\"COMBINING {classes} FROM TRAINING AND VALIDATION DATA\")\n",
    "\n",
    "        random_images = {}\n",
    "\n",
    "        # Using ThreadPoolExecutor to speed up the process of fetching images from each class in parallel\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            results = executor.map(process_class, classes)\n",
    "\n",
    "        # Store the random image results for each class into the dictionary\n",
    "        for cls, images in zip(classes, results):\n",
    "            random_images[cls] = images\n",
    "            COLLECT_AND_COMBINE_IMAGES_LOGGER.info(f\"Total {log_manager.color_text(cls, 'yellow')} taken: {log_manager.color_text(len(random_images[cls]), 'yellow')}\")\n",
    "\n",
    "        # Merge all image paths from all classes\n",
    "        all_images_paths = sum(random_images.values(), [])\n",
    "        all_images_paths = [str(path) for path in all_images_paths]\n",
    "        custom_title_print(f\"Total images taken: {len(all_images_paths)}\")\n",
    "\n",
    "        return all_images_paths\n",
    "\n",
    "    except Exception as e:\n",
    "        raise SIBIClassificationException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[ 2024-11-29 18:22:56 ] DataPreprocessingLogger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting and combining images from training and validation folders...\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - COLLECT ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] FROM TRAINING DATA\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mA\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mB\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mC\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mD\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mE\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mF\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mG\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mH\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mI\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mJ\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mK\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mL\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mM\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mN\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mO\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mP\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mQ\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mR\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mS\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mT\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mU\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mV\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mW\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mX\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mY\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Collecting from class \u001b[93mZ\u001b[0m: \u001b[93m200\u001b[0m paths found.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ==================================================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - COLLECT ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] FROM VALIDATION DATA\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - No classes are retrieved from directory.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ==================================================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - COLLECT ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] FROM TEST DATA\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - No classes are retrieved from directory.\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ==================================================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - COMBINING ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'] FROM TRAINING AND VALIDATION DATA\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mA\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mB\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mC\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mD\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mE\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mF\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mG\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mH\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mI\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mJ\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mK\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mL\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mM\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mN\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mO\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mP\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mQ\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mR\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mS\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mT\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mU\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mV\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mW\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mX\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mY\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:57 ] collect_and_combine_images_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total \u001b[93mZ\u001b[0m taken: \u001b[93m200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:22:56 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - =============TOTAL IMAGES TAKEN: 5200=============\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "LOGGER.info(\"Collecting and combining images from training and validation folders...\")\n",
    "\n",
    "all_images_paths = collect_and_combine_images(\n",
    "    classes = DATA_PREPROCESSING_CONFIG.label_list,\n",
    "    train_path  = DATA_INGESTION_CONFIG.data_download_store_train_dir_path,\n",
    "    pattern_regex = DATA_PREPROCESSING_CONFIG.image_extension_regex,\n",
    "    seed= DATA_PREPROCESSING_CONFIG.seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display file paths info\n",
    "\n",
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePathInfo:\n",
    "    \"\"\"\n",
    "    The FilePathInfo class is used to display detailed information about file paths on a dataset, including\n",
    "    file name, extension, file size, and label (if any). This class also supports the use of different\n",
    "    size units such as 'bytes', 'kb', 'mb', and 'gb'.\n",
    "    Args:\n",
    "        unit_file_size (str, optional): The unit to display the file size ('bytes', 'kb', 'mb', 'gb'). Default is 'bytes'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            unit_file_size='bytes'\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize the FilePathInfo class with the given file size unit.\n",
    "        Args:\n",
    "            unit_file_size (str, optional): The unit to display the file size ('bytes', 'kb', 'mb', 'gb'). Default is 'bytes'.\n",
    "        \"\"\"\n",
    "        self.unit_file_size = unit_file_size.lower()\n",
    "        self.units = ['bytes', 'kb', 'mb', 'gb']\n",
    "        if self.unit_file_size not in self.units:\n",
    "            raise ValueError(f\"Invalid unit. Choose from {self.units}.\")\n",
    "\n",
    "    def show_train_files_path_info(\n",
    "            self, \n",
    "            files_path_data, \n",
    "            is_random=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Display detailed information about file paths in the training dataset.\n",
    "        Args:\n",
    "            files_path_data(tf.data.Dataset): The dataset containing the file paths.\n",
    "            is_labeled (bool, optional): Indicates whether the dataset has a label. Default is True.\n",
    "            is_random (bool, optional): Indicates whether the dataset needs to be randomized before displaying. Default is False.\n",
    "        Returns:\n",
    "            int: The label index on the file path, if the dataset has a label.\n",
    "        \"\"\"\n",
    "\n",
    "        files_path_data_plot = random.choice(files_path_data) if is_random else files_path_data[0]\n",
    "        label_index = self._display_path_info(files_path_data_plot, is_labeled=True)\n",
    "        return label_index\n",
    "\n",
    "    def show_test_files_path_info(\n",
    "            self, \n",
    "            files_path_data, \n",
    "            is_random=False\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Display detailed information about file paths in the testing dataset.\n",
    "        Args:\n",
    "            files_path_data(tf.data.Dataset): The dataset containing the file paths.\n",
    "            is_random (bool, optional): Indicates whether the dataset needs to be randomized before display. Default is False.\n",
    "        \"\"\"\n",
    "\n",
    "        files_path_data_plot = random.choice(files_path_data) if is_random else files_path_data[0]\n",
    "        self._display_path_info(files_path_data_plot, is_labeled=False)\n",
    "\n",
    "    def _display_path_info(\n",
    "            self, \n",
    "            file_path, \n",
    "            is_labeled\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Displays the full information of the selected file path, including the file name, extension, size, and label if any.\n",
    "        Args:\n",
    "            files_path_data_plot(tf.data.Dataset): A subset of the dataset to be displayed.\n",
    "            is_labeled (bool): Indicates whether the dataset has a label.\n",
    "        Returns:\n",
    "            int: The label index on the file path if the dataset is labeled.\n",
    "        \"\"\"\n",
    "        custom_title_print(' PATH INFO ')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'File Path: {log_manager.color_text(file_path, COLOR_TEXT)}')\n",
    "\n",
    "        split_file_path = file_path.split(os.path.sep)\n",
    "        self._display_split_file_path(split_file_path)\n",
    "\n",
    "        if is_labeled:\n",
    "            kind_data = split_file_path[-3]\n",
    "            index_label = self._display_kind_data_info(split_file_path, kind_data)\n",
    "            self._display_file_info(split_file_path, file_path)\n",
    "            return index_label\n",
    "        else:\n",
    "            self._display_file_info(split_file_path, file_path)\n",
    "\n",
    "    def _display_split_file_path(\n",
    "            self, \n",
    "            split_file_path\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Displays the path of the split file and the index of each section.\n",
    "        Args:\n",
    "            split_file_path(tf.Tensor): The path of the split file.\n",
    "        \"\"\"\n",
    "        custom_title_print(' SPLIT FILE PATH ')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'Split File Path: {log_manager.color_text(split_file_path, COLOR_TEXT)}')\n",
    "\n",
    "        custom_title_print(' INDEXED PATH ')\n",
    "        result = {value: f'Index -> {index}' for index, value in enumerate(split_file_path)}\n",
    "        for key, value in result.items():\n",
    "            FILE_PATH_INFO_LOGGER.info(f'{log_manager.color_text(value, COLOR_TEXT)}: {log_manager.color_text(key, COLOR_TEXT)}')\n",
    "\n",
    "    def _display_kind_data_info(\n",
    "            self, \n",
    "            split_file_path, \n",
    "            kind_data\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Displays the index and label of the data based on its type.\n",
    "        Args:\n",
    "            split_file_path(tf.Tensor): The path of the split file.\n",
    "            kind_data (str): The type of data in the file path.\n",
    "        Returns:\n",
    "            int: The index of the label in the file path.\n",
    "        \"\"\"\n",
    "        custom_title_print(f' KIND DATA INDEX {kind_data} ')\n",
    "        index = split_file_path.index(kind_data)\n",
    "        FILE_PATH_INFO_LOGGER.info(f'Index of \"{log_manager.color_text(kind_data, COLOR_TEXT)}\": {log_manager.color_text(index, COLOR_TEXT)}')\n",
    "\n",
    "        index_label = index + 1\n",
    "        custom_title_print(' INDEX LABEL ')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'Index Label: {log_manager.color_text(index_label, COLOR_TEXT)}')\n",
    "\n",
    "        custom_title_print(' LABEL ')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'Label: {log_manager.color_text(split_file_path[index_label], COLOR_TEXT)}')\n",
    "\n",
    "        return index_label\n",
    "\n",
    "    def _display_file_info(\n",
    "            self, \n",
    "            split_file_path, \n",
    "            file_path\n",
    "        ):\n",
    "\n",
    "        \"\"\"\n",
    "        Displays detailed information about the file such as name, extension, and file size.\n",
    "        Args:\n",
    "            split_file_path(tf.Tensor): The path of the split file.\n",
    "            file_path (tf.Tensor): The path of the file.\n",
    "        \"\"\"\n",
    "        file_name = split_file_path[-1]\n",
    "        custom_title_print(' FILE NAME ')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'File Name: {log_manager.color_text(file_name, COLOR_TEXT)}')\n",
    "\n",
    "        file_extension = os.path.splitext(file_name)[1]\n",
    "        custom_title_print(' FILE EXTENSION ')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'File Extension: {log_manager.color_text(file_extension, COLOR_TEXT)}')\n",
    "\n",
    "        image_size = Image.open(file_path).size\n",
    "        custom_title_print(' IMAGE SIZE (PX)')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'Image Size: width={log_manager.color_text(image_size[0], COLOR_TEXT)}, height={log_manager.color_text(image_size[1], COLOR_TEXT)}')\n",
    "\n",
    "        file_size = os.path.getsize(file_path)\n",
    "        file_size = self._format_file_size(file_size)\n",
    "        custom_title_print(' FILE SIZE ')\n",
    "        FILE_PATH_INFO_LOGGER.info(f'File Size: {log_manager.color_text(file_size, COLOR_TEXT)} {log_manager.color_text(self.unit_file_size, COLOR_TEXT)}')\n",
    "\n",
    "\n",
    "    def _format_file_size(\n",
    "            self, \n",
    "            size\n",
    "        ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Formats the file size according to the selected unit.\n",
    "        Args:\n",
    "            size (int): File size in bytes.\n",
    "        Returns:\n",
    "            str: The size of the formatted file.\n",
    "        \"\"\"\n",
    "\n",
    "        match self.unit_file_size:\n",
    "            case 'bytes':\n",
    "                size = size\n",
    "            case 'kb':\n",
    "                size /= 1024\n",
    "            case 'mb':\n",
    "                size /= 1024 ** 2\n",
    "            case 'gb':\n",
    "                size /= 1024 ** 3\n",
    "            case _:\n",
    "                raise ValueError(f\"Invalid unit. Choose from {self.units}.\")\n",
    "        \n",
    "        size = round(size, 4)\n",
    "\n",
    "        return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first data: artifacts/<model_name>/data_ingestion/SIBI_dataset/Train/A/image_A_(1723068087.4824092).jpg\n",
      "number of data: 5200\n"
     ]
    }
   ],
   "source": [
    "print(f'first data: {all_images_paths[0]}')\n",
    "print(f'number of data: {len(all_images_paths)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info  = FilePathInfo(unit_file_size='kb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - =================== PATH INFO ====================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - File Path: \u001b[93martifacts/<model_name>/data_ingestion/SIBI_dataset/Train/W/image_W_(1723083370.198182).jpg\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ================ SPLIT FILE PATH =================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Split File Path: \u001b[93m['artifacts', '<model_name>', 'data_ingestion', 'SIBI_dataset', 'Train', 'W', 'image_W_(1723083370.198182).jpg']\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ================== INDEXED PATH ==================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - \u001b[93mIndex -> 0\u001b[0m: \u001b[93martifacts\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - \u001b[93mIndex -> 1\u001b[0m: \u001b[93m<model_name>\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - \u001b[93mIndex -> 2\u001b[0m: \u001b[93mdata_ingestion\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - \u001b[93mIndex -> 3\u001b[0m: \u001b[93mSIBI_dataset\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - \u001b[93mIndex -> 4\u001b[0m: \u001b[93mTrain\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - \u001b[93mIndex -> 5\u001b[0m: \u001b[93mW\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - \u001b[93mIndex -> 6\u001b[0m: \u001b[93mimage_W_(1723083370.198182).jpg\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ============= KIND DATA INDEX TRAIN ==============\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Index of \"\u001b[93mTrain\u001b[0m\": \u001b[93m4\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ================== INDEX LABEL ===================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Index Label: \u001b[93m5\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ===================== LABEL ======================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Label: \u001b[93mW\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - =================== FILE NAME ====================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - File Name: \u001b[93mimage_W_(1723083370.198182).jpg\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ================= FILE EXTENSION =================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - File Extension: \u001b[93m.jpg\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ================= IMAGE SIZE (PX)=================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Image Size: width=\u001b[93m224\u001b[0m, height=\u001b[93m224\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - =================== FILE SIZE ====================\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:19 ] file_path_info_logger\u001b[0m - \u001b[32mINFO\u001b[0m - File Size: \u001b[93m17.6436\u001b[0m \u001b[93mkb\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "LABEL_INDEX = file_info.show_train_files_path_info(all_images_paths, is_random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Pytorch Dataset\n",
    "\n",
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, file_paths, label_index):\n",
    "        \"\"\"\n",
    "        Dataset for a list of image files.\n",
    "        :param file_paths: List paths to images.\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.label_index = label_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        split_text = self.file_paths[idx].split(os.path.sep)\n",
    "        label = split_text[self.label_index]\n",
    "        return self.file_paths[idx], label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_paths = ImageDataset(all_images_paths, LABEL_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('artifacts/<model_name>/data_ingestion/SIBI_dataset/Train/A/image_A_(1723068087.4824092).jpg',\n",
       " 'A')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_paths[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplitter:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def split_train_valid_test(self, dataset, split_ratio=None, shuffle=True, seed=42):\n",
    "        if split_ratio is None or len(split_ratio) < 2:\n",
    "            raise ValueError(\"split_ratio must be of the form (train_ratio, val_ratio).\")\n",
    "\n",
    "        train_ratio, val_ratio = split_ratio\n",
    "        test_ratio = max(1.0 - (train_ratio + val_ratio), 0)\n",
    "\n",
    "        total_ratio = round(sum((train_ratio, val_ratio, test_ratio)), 2)\n",
    "        if total_ratio != 1.0:\n",
    "            raise ValueError(\"[ERROR] split_ratio must sum to 1.0.\")\n",
    "\n",
    "        dataset_size = len(dataset)\n",
    "        labels = [data[1] for data in dataset]  # Assuming dataset[i] = (data, label)\n",
    "\n",
    "        # Stratified split for train and remaining (val + test)\n",
    "        train_idx, temp_idx, _, temp_labels = train_test_split(\n",
    "            list(range(dataset_size)),\n",
    "            labels,\n",
    "            test_size=(val_ratio + test_ratio) if (val_ratio + test_ratio) > 0 else None,\n",
    "            stratify=labels if labels else None,\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "        if test_ratio > 0:  # If test_ratio > 0, perform stratified split for val and test\n",
    "            val_size_ratio = val_ratio / (val_ratio + test_ratio)\n",
    "            val_idx, test_idx, _, _ = train_test_split(\n",
    "                temp_idx,\n",
    "                temp_labels,\n",
    "                test_size=(1 - val_size_ratio),\n",
    "                stratify=temp_labels if temp_labels else None,\n",
    "                random_state=seed,\n",
    "            )\n",
    "        else:  # If test_ratio == 0, assign all remaining data to validation\n",
    "            val_idx = temp_idx\n",
    "            test_idx = []\n",
    "\n",
    "        train_dataset = Subset(dataset, train_idx)\n",
    "        val_dataset = Subset(dataset, val_idx)\n",
    "        test_dataset = Subset(dataset, test_idx)\n",
    "\n",
    "        self._display_info(\n",
    "            dataset_size=dataset_size,\n",
    "            train_dataset=train_dataset,\n",
    "            valid_dataset=val_dataset,\n",
    "            test_dataset=test_dataset,\n",
    "            shuffle=shuffle,\n",
    "        )\n",
    "\n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "    def _display_info(self, dataset_size, train_dataset, valid_dataset, test_dataset, shuffle):\n",
    "        \"\"\"\n",
    "        Displays information about the split dataset.\n",
    "        \"\"\"\n",
    "        train_ratio = len(train_dataset) / dataset_size\n",
    "        valid_ratio = len(valid_dataset) / dataset_size\n",
    "        test_ratio = len(test_dataset) / dataset_size\n",
    "\n",
    "        SPLIT_TRAIN_VALID_TEST_LOGGER.info(f\"Total number of data: {log_manager.color_text(dataset_size, COLOR_TEXT)}\")\n",
    "        SPLIT_TRAIN_VALID_TEST_LOGGER.info(f\"Shuffle status: {log_manager.color_text(shuffle, COLOR_TEXT)}\")\n",
    "        SPLIT_TRAIN_VALID_TEST_LOGGER.info(f\"Training Dataset: {log_manager.color_text(len(train_dataset), COLOR_TEXT)} ({log_manager.color_text(f'{train_ratio * 100:.2f}%', COLOR_TEXT)})\")\n",
    "        SPLIT_TRAIN_VALID_TEST_LOGGER.info(f\"Validation Dataset: {log_manager.color_text(len(valid_dataset), COLOR_TEXT)} ({log_manager.color_text(f'{valid_ratio * 100:.2f}%', COLOR_TEXT)})\")\n",
    "        SPLIT_TRAIN_VALID_TEST_LOGGER.info(f\"Test Dataset: {log_manager.color_text(len(test_dataset), COLOR_TEXT)} ({log_manager.color_text(f'{test_ratio * 100:.2f}%', COLOR_TEXT)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = DatasetSplitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[ 2024-11-29 18:23:50 ] split_train_valid_test_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Total number of data: \u001b[93m5200\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:50 ] split_train_valid_test_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Shuffle status: \u001b[93mTrue\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:50 ] split_train_valid_test_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Training Dataset: \u001b[93m4160\u001b[0m (\u001b[93m80.00%\u001b[0m)\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:50 ] split_train_valid_test_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Validation Dataset: \u001b[93m1040\u001b[0m (\u001b[93m20.00%\u001b[0m)\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:23:50 ] split_train_valid_test_logger\u001b[0m - \u001b[32mINFO\u001b[0m - Test Dataset: \u001b[93m0\u001b[0m (\u001b[93m0.00%\u001b[0m)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_pt_paths, val_pt_paths, _ = splitter.split_train_valid_test(\n",
    "    pytorch_paths, \n",
    "    split_ratio=(0.8, 0.2),\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "\n",
    "### function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_distribution_torch(\n",
    "        dataset=None, \n",
    "        class_labels=None\n",
    "    ) -> tuple:\n",
    "    \"\"\"\n",
    "    Menghitung distribusi kelas dan class weights untuk dataset PyTorch.\n",
    "\n",
    "    :param dataset: Dataset PyTorch (torch.utils.data.Dataset atau torch.utils.data.Subset)\n",
    "    :param class_labels: Daftar label kelas (list).\n",
    "    :return: Tuple (class_counts, class_weights).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Mengambil semua label dari dataset\n",
    "        class_names = [data[1] for data in dataset]\n",
    "        \n",
    "        # Hitung jumlah tiap kelas\n",
    "        class_counts = Counter(class_names)\n",
    "\n",
    "        # Konversi ke indeks berdasarkan urutan class_labels\n",
    "        class_indices = [class_labels.index(name) for name in class_names]\n",
    "\n",
    "        # Hitung class weights menggunakan sklearn\n",
    "        class_weight_values = compute_class_weight(\n",
    "            class_weight='balanced',\n",
    "            classes=np.unique(class_indices),\n",
    "            y=class_indices\n",
    "        )\n",
    "\n",
    "        # Buat dictionary untuk class weights\n",
    "        class_weights = {i: weight for i, weight in enumerate(class_weight_values)}\n",
    "\n",
    "        return class_counts, class_weights\n",
    "\n",
    "    except Exception as e:\n",
    "        raise SIBIClassificationException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(\n",
    "        distribution: collections.Counter=None\n",
    "    ) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Prints the class distribution to the screen.\n",
    "    Args:\n",
    "        distribution(collections.Counter): A Counter object containing the class distribution.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for class_name, count in sorted(distribution.items()):\n",
    "            CLASS_DISTRIBUTION_LOGGER.info(f\"class {log_manager.color_text(class_name, COLOR_TEXT)}: {log_manager.color_text(count, COLOR_TEXT)} items\")\n",
    "\n",
    "    except Exception as e:\n",
    "        raise SIBIClassificationException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_distribution, class_weights = calculate_class_distribution_torch(\n",
    "    dataset=train_pt_paths, \n",
    "    class_labels=DATA_PREPROCESSING_CONFIG.label_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_class_distribution, _ = calculate_class_distribution_torch(\n",
    "    dataset=val_pt_paths, \n",
    "    class_labels=DATA_PREPROCESSING_CONFIG.label_list\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[ 2024-11-29 18:24:38 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - =========CLASS DISTRIBUTION ON TRAIN SET:=========\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mA\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mB\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mC\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mD\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mE\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mF\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mG\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mH\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mI\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mJ\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mK\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mL\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mM\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mN\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mO\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mP\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mQ\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mR\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mS\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mT\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mU\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mV\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mW\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mX\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mY\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mZ\u001b[0m: \u001b[93m160\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] title_print_logger\u001b[0m - \u001b[32mINFO\u001b[0m - ======CLASS DISTRIBUTION IN VALIDATION SET:=======\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mA\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mB\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mC\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mD\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mE\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mF\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mG\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mH\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mI\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mJ\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mK\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mL\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mM\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mN\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mO\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mP\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mQ\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mR\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mS\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mT\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mU\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mV\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mW\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mX\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mY\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n",
      "\u001b[32m[ 2024-11-29 18:24:38 ] class_distribution_logger\u001b[0m - \u001b[32mINFO\u001b[0m - class \u001b[93mZ\u001b[0m: \u001b[93m40\u001b[0m items\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "custom_title_print(\"Class distribution on Train set:\")\n",
    "print_class_distribution(train_class_distribution)\n",
    "\n",
    "custom_title_print(\"Class distribution in Validation set:\")\n",
    "print_class_distribution(valid_class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_manager.clean_log_file()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
